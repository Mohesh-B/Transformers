{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer"
      ],
      "metadata": {
        "id": "frdumDFDm8Jv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hh1fNXZdknti"
      },
      "outputs": [],
      "source": [
        "class FeedForward(Layer):\n",
        "    def __init__(self, d_ff, d_model, **kwargs):\n",
        "        super(FeedForward, self).__init__(**kwargs)\n",
        "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
        "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
        "        self.activation = ReLU()  # ReLU activation layer\n",
        "\n",
        "    def call(self, x):\n",
        "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
        "        x_fc1 = self.fully_connected1(x)\n",
        "\n",
        "        return self.fully_connected2(self.activation(x_fc1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNormalization(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AddNormalization, self).__init__(**kwargs)\n",
        "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
        "\n",
        "\n",
        "    def call(self, x, sublayer_x):\n",
        "        # The sublayer input and output need to be of the same shape to be summed\n",
        "        add = x + sublayer_x\n",
        "\n",
        "        # Apply layer normalization to the sum\n",
        "        return self.layer_norm(add)"
      ],
      "metadata": {
        "id": "1sDmG2cklPOP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.add_norm1 = AddNormalization()\n",
        "        self.feed_forward = FeedForward(d_ff, d_model)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.add_norm2 = AddNormalization()\n",
        "\n",
        "    def call(self, x, padding_mask, training):\n",
        "        # Multi-head attention layer\n",
        "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        multihead_output = self.dropout1(multihead_output, training=training)\n",
        "\n",
        "        # Followed by an Add & Norm layer\n",
        "        addnorm_output = self.add_norm1(x, multihead_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Followed by a fully connected layer\n",
        "        feedforward_output = self.feed_forward(addnorm_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in another dropout layer\n",
        "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
        "\n",
        "        # Followed by another Add & Norm layer\n",
        "        return self.add_norm2(addnorm_output, feedforward_output)\n"
      ],
      "metadata": {
        "id": "oZ7KRxePlkIg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.dropout = Dropout(rate)\n",
        "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        "        ...\n",
        "    def call(self, input_sentence, padding_mask, training):\n",
        "        # Generate the positional encoding\n",
        "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        x = self.dropout(pos_encoding_output, training=training)\n",
        "\n",
        "        # Pass on the positional encoded values to each encoder layer\n",
        "        for i, layer in enumerate(self.encoder_layer):\n",
        "            x = layer(x, padding_mask, training)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "EBHYeB0ulr_3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "from keras.backend import softmax\n",
        "\n",
        "# Implementing the Scaled-Dot Product Attention\n",
        "class DotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "\n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        "\n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = softmax(scores)\n",
        "\n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)\n",
        "\n",
        "# Implementing the Multi-Head Attention\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
        "        self.heads = h  # Number of attention heads to use\n",
        "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
        "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
        "        self.d_model = d_model  # Dimensionality of the model\n",
        "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
        "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
        "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
        "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
        "\n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "        else:\n",
        "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
        "        return x\n",
        "\n",
        "    def call(self, queries, keys, values, mask=None):\n",
        "        # Rearrange the queries to be able to compute all heads in parallel\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange the keys to be able to compute all heads in parallel\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange the values to be able to compute all heads in parallel\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange back the output into concatenated form\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
        "\n",
        "        # Apply one final linear projection to the output to generate the multi-head attention\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "        return self.W_o(output)"
      ],
      "metadata": {
        "id": "oLh60z8Xp5gw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddingFixedWeights(Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
        "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
        "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
        "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)\n",
        "        self.word_embedding_layer = Embedding(\n",
        "            input_dim=vocab_size, output_dim=output_dim,\n",
        "            weights=[word_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "        self.position_embedding_layer = Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim,\n",
        "            weights=[position_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def get_position_encoding(self, seq_len, d, n=10000):\n",
        "        P = np.zeros((seq_len, d))\n",
        "        for k in range(seq_len):\n",
        "            for i in np.arange(int(d/2)):\n",
        "                denominator = np.power(n, 2*i/d)\n",
        "                P[k, 2*i] = np.sin(k/denominator)\n",
        "                P[k, 2*i+1] = np.cos(k/denominator)\n",
        "        return P\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ],
      "metadata": {
        "id": "YITi3f3Zqoql"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
        "#from multihead_attention import MultiHeadAttention\n",
        "#from positional_encoding import PositionEmbeddingFixedWeights\n",
        "\n",
        "# Implementing the Add & Norm Layer\n",
        "class AddNormalization(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AddNormalization, self).__init__(**kwargs)\n",
        "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
        "\n",
        "    def call(self, x, sublayer_x):\n",
        "        # The sublayer input and output need to be of the same shape to be summed\n",
        "        add = x + sublayer_x\n",
        "\n",
        "        # Apply layer normalization to the sum\n",
        "        return self.layer_norm(add)\n",
        "\n",
        "# Implementing the Feed-Forward Layer\n",
        "class FeedForward(Layer):\n",
        "    def __init__(self, d_ff, d_model, **kwargs):\n",
        "        super(FeedForward, self).__init__(**kwargs)\n",
        "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
        "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
        "        self.activation = ReLU()  # ReLU activation layer\n",
        "\n",
        "    def call(self, x):\n",
        "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
        "        x_fc1 = self.fully_connected1(x)\n",
        "\n",
        "        return self.fully_connected2(self.activation(x_fc1))\n",
        "\n",
        "# Implementing the Encoder Layer\n",
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.add_norm1 = AddNormalization()\n",
        "        self.feed_forward = FeedForward(d_ff, d_model)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.add_norm2 = AddNormalization()\n",
        "\n",
        "    def call(self, x, padding_mask, training):\n",
        "        # Multi-head attention layer\n",
        "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in a dropout layer\n",
        "        multihead_output = self.dropout1(multihead_output, training=training)\n",
        "\n",
        "        # Followed by an Add & Norm layer\n",
        "        addnorm_output = self.add_norm1(x, multihead_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Followed by a fully connected layer\n",
        "        feedforward_output = self.feed_forward(addnorm_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "\n",
        "        # Add in another dropout layer\n",
        "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
        "\n",
        "        # Followed by another Add & Norm layer\n",
        "        return self.add_norm2(addnorm_output, feedforward_output)\n",
        "\n",
        "# Implementing the Encoder\n",
        "class Encoder(Layer):\n",
        "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.dropout = Dropout(rate)\n",
        "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        "\n",
        "    def call(self, input_sentence, padding_mask, training):\n",
        "        # Generate the positional encoding\n",
        "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        "        #print(\"asda: \" ,pos_encoding_output)\n",
        "        # Add in a dropout layer\n",
        "        x = self.dropout(pos_encoding_output, training=training)\n",
        "       # print(\"dfsda :\",x);\n",
        "        # Pass on the positional encoded values to each encoder layer\n",
        "        for i, layer in enumerate(self.encoder_layer):\n",
        "            x = layer(x, padding_mask, training)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "WszVNllSlzXg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lFwlAyOeVeY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "import numpy as np\n",
        "from keras.layers import Embedding\n",
        "import tensorflow as tf\n",
        "\n",
        "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
        "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
        "n = 6  # Number of layers in the encoder stack\n",
        "\n",
        "batch_size = 64  # Batch size from the training process\n",
        "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
        "\n",
        "input_seq = random.random((batch_size, input_seq_length))\n",
        "\n",
        "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
        "print(encoder(input_seq, None, True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JBvuz5-mDl4",
        "outputId": "c078e9a5-5397-4ebf-ed5f-debea0ce5db2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-1.8498997e-01  6.7723460e-02  1.4434396e+00 ...  9.8369855e-01\n",
            "   -8.1869596e-01  9.8321266e-02]\n",
            "  [ 5.4426450e-02 -2.8766522e-01  1.3032812e+00 ...  1.3790224e+00\n",
            "   -1.3547343e+00  2.9888454e-01]\n",
            "  [ 4.6138456e-01 -2.6869839e-01  1.4672137e+00 ...  8.6038989e-01\n",
            "   -8.8809240e-01 -6.3581485e-01]\n",
            "  [ 5.5991340e-01 -4.9371690e-01  1.2400427e+00 ...  4.6810234e-01\n",
            "   -1.0645489e+00 -2.2888459e-01]\n",
            "  [ 9.5920280e-02 -5.4339743e-01  2.0941479e-02 ...  1.6892046e+00\n",
            "   -4.8837487e-02  1.6299292e+00]]\n",
            "\n",
            " [[ 1.6652581e-01  6.1717570e-01  1.5031414e+00 ...  7.0579797e-01\n",
            "   -7.2269982e-01 -1.6539299e-01]\n",
            "  [ 4.3102217e-01  9.8182744e-01  1.5778728e+00 ...  8.9486653e-01\n",
            "   -8.7666023e-01 -5.4807585e-01]\n",
            "  [ 6.9046432e-01  5.2119547e-01  1.4366618e+00 ...  9.5825362e-01\n",
            "   -1.1841877e+00 -1.8705951e-01]\n",
            "  [ 1.7361419e-01  3.7426755e-01  1.6818933e+00 ...  7.1230692e-01\n",
            "   -2.2003555e+00 -3.1125763e-01]\n",
            "  [ 2.6498130e-01 -4.1303724e-01  1.1190710e+00 ...  1.0001894e+00\n",
            "   -3.6662650e-01 -2.5830129e-01]]\n",
            "\n",
            " [[ 2.5459707e-01 -2.0737445e-01  1.2069516e+00 ...  6.0273474e-01\n",
            "   -1.4642390e+00  2.5583640e-01]\n",
            "  [ 6.4454186e-01  9.0098575e-02  1.7625792e+00 ...  1.9081329e+00\n",
            "   -3.7873557e-01  5.8063012e-01]\n",
            "  [ 3.6396086e-01 -6.1291593e-01  1.3413198e+00 ...  7.3093706e-01\n",
            "   -6.0080010e-01  5.4981750e-01]\n",
            "  [-2.3143449e-01 -7.2856468e-01  2.1229403e+00 ...  3.3284056e-01\n",
            "   -6.1890006e-01 -5.0805237e-02]\n",
            "  [-1.9817781e-03 -8.8211769e-01  9.3192250e-01 ...  2.8020751e-01\n",
            "   -9.5687008e-01  4.6706945e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.7754869e-01  4.0033892e-02  1.6217501e+00 ...  1.0661838e+00\n",
            "    4.0204626e-01  6.3003510e-01]\n",
            "  [ 7.2070926e-02 -2.7894940e-02  9.2172170e-01 ...  1.3534358e+00\n",
            "   -6.1279637e-01  1.1918842e+00]\n",
            "  [-1.1801935e-01 -3.2079965e-01  1.6096675e+00 ...  1.3624873e+00\n",
            "   -7.4620312e-01  8.7022871e-01]\n",
            "  [ 2.2018681e-01 -1.6869615e-01  1.8981314e+00 ...  1.1180743e+00\n",
            "   -7.0029593e-01  1.9958360e-02]\n",
            "  [ 4.3784428e-01 -5.1414120e-01  1.5278705e+00 ...  8.1226271e-01\n",
            "   -7.9923487e-01  2.9649270e-01]]\n",
            "\n",
            " [[ 6.2443513e-01 -6.9034994e-01  1.6599658e+00 ...  1.4711530e+00\n",
            "   -2.4312527e+00 -2.0643542e-02]\n",
            "  [ 4.3501699e-01  3.9020833e-02  1.3944966e+00 ...  3.5061743e-03\n",
            "   -1.3184922e+00 -4.9923322e-01]\n",
            "  [ 7.8507102e-01 -4.7918033e-02  1.5313530e+00 ...  1.0749236e+00\n",
            "   -1.2556534e-01 -2.9991999e-01]\n",
            "  [-8.9839615e-02 -3.7438557e-01  1.4238144e+00 ...  4.4435999e-01\n",
            "   -6.9131976e-01 -2.9544744e-01]\n",
            "  [-7.8980349e-02 -9.1567576e-02  1.8851485e+00 ...  9.8668456e-01\n",
            "   -1.0643793e+00  7.0649201e-01]]\n",
            "\n",
            " [[ 8.1638366e-01  4.2949621e-02  1.5626345e+00 ...  1.2845649e+00\n",
            "   -2.0551799e-01  3.8535647e-02]\n",
            "  [ 3.2969126e-01 -9.1931835e-02  1.3959311e+00 ...  1.4451816e+00\n",
            "   -1.2468168e+00 -1.0487621e+00]\n",
            "  [ 2.4843530e-01  3.9869493e-01  1.0984309e+00 ...  7.8583598e-01\n",
            "   -1.1926715e+00 -3.6945534e-01]\n",
            "  [ 4.9545237e-01 -6.4072281e-01  1.8464487e+00 ...  1.0111918e+00\n",
            "   -1.2811409e-01 -7.5931591e-01]\n",
            "  [-4.0611126e-02  2.0906523e-01  1.5790644e+00 ...  8.8433397e-01\n",
            "   -7.1871108e-01 -1.1780181e+00]]], shape=(64, 5, 512), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}